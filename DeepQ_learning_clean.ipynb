{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from state import get_state, queue\n",
    "import os\n",
    "import traci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "sumo_bin = os.getenv(\"SUMO\")\n",
    "sumo_gui_bin = os.getenv(\"SUMO-GUI\")\n",
    "simulConfig = os.getenv(\"SIMUL-CONFIG\")\n",
    "\n",
    "print(sumoBinary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.random import set_seed\n",
    "from collections import deque\n",
    "from tensorflow import keras\n",
    "from tensorflow import reduce_sum, reduce_mean, one_hot, GradientTape\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)  # extra code – ensures reproducibility on the CPU\n",
    "\n",
    "input_shape = [48]  # == env.observation_space.shape\n",
    "n_outputs = 4  # == env.action_space.n\n",
    "#[((Phase(duration=30.0, state='GGrGrrGGrGrr', minDur=30.0, maxDur=30.0), Phase(duration=20.0, state='grGgrrgrGgrr', minDur=20.0, maxDur=20.0), Phase(duration=30.0, state='GrrGGrGrrGGr', minDur=30.0, maxDur=30.0), Phase(duration=20.0, state='grrgrGgrrgrG', minDur=20.0, maxDur=20.0)), [0, 2, 4, 6])]\n",
    "model_action = Sequential([\n",
    "    layers.Dense(32,activation=tf.keras.layers.LeakyReLU(alpha=0.01), input_shape=input_shape),\n",
    "    layers.Dense(32, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n",
    "    layers.Dense(n_outputs, activation= 'linear')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)  # random action\n",
    "    else:\n",
    "        Q_values = model_action.predict(state[np.newaxis], verbose=0)[0]\n",
    "        print(f\"state : {state}\")\n",
    "        print(f\"Q_values : {Q_values}\")\n",
    "        return Q_values.argmax()  # optimal action according to the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    batch = [replay_buffer[index] for index in indices]\n",
    "    states, actions, rewards, next_states = [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(4)\n",
    "    ]\n",
    "    return states, actions, rewards, next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_factor = 0.5\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.05)\n",
    "loss_fn = MeanSquaredError()\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states = experiences  # a changer\n",
    "    next_Q_values = model_action.predict(next_states, verbose=0)\n",
    "    max_next_Q_values = next_Q_values.max(axis=1)\n",
    "    # runs = 1.0 - (dones | truncateds)  # episode is not done or truncated\n",
    "    target_Q_values = rewards + discount_factor * max_next_Q_values\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = one_hot(actions, n_outputs)\n",
    "    with GradientTape() as tape:\n",
    "        all_Q_values = model_action(states)\n",
    "        Q_values = reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "\n",
    "    grads = tape.gradient(loss, model_action.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model_action.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "best_score = 0\n",
    "reward = 0\n",
    "total_reward = 0\n",
    "list_values = []\n",
    "weights = []\n",
    "wait_times = []\n",
    "replay_buffer = deque(maxlen=10000)\n",
    "\n",
    "\n",
    "sumoCmd = [sumo_bin, \"-c\", simulConfig, \"--start\"]\n",
    "sumoCmd = [sumo_bin, \"-c\", simulConfig, \"--start\", \"--no-warnings\"]\n",
    "\n",
    "for episode in range(300):\n",
    "    if traci.isLoaded():\n",
    "        traci.close()\n",
    "    traci.start(sumoCmd)\n",
    "    lane_ids =  traci.lane.getIDList()\n",
    "    # print(lane_ids[0])\n",
    "\n",
    "    trafic_light_ids = traci.trafficlight.getIDList()\n",
    "\n",
    "    # state = np.array(queue(lane_ids))\n",
    "    state=np.array(get_state(lane_ids))\n",
    "    action=-1\n",
    "    # print(state)\n",
    "    wait_times.append(0)\n",
    "    for step in range(10000): ## TO CHANGED\n",
    "        epsilon = max(1 - episode / 300, 0.01)\n",
    "\n",
    "        if step%1000 == 0:\n",
    "            #print(f\"longeur du buffer :{len(replay_buffer)}\")\n",
    "            ########################################################################\n",
    "            ##Calcul de la reward\n",
    "            # next_state = np.array(queue(lane_ids))\n",
    "            next_state = np.array(get_state(lane_ids))\n",
    "            # reward = calculate_reward(values, reward, total_reward)[0]\n",
    "            # reward=(np.sum(state)-np.sum(next_state))\n",
    "            reward = np.sum(state[:24])- np.sum(next_state[:24])\n",
    "            replay_buffer.append((state, action, reward, next_state))\n",
    "\n",
    "            list_values.append(queue(lane_ids))\n",
    "            #########################################################################\n",
    "            state=next_state\n",
    "            action = epsilon_greedy_policy(state, epsilon)\n",
    "            #print(\"action\", action)\n",
    "            traci.trafficlight.setPhase(trafic_light_ids[0],2*action)\n",
    "\n",
    "            if len(replay_buffer) >= batch_size*10:\n",
    "                training_step(batch_size)\n",
    "                # new_weights = model_action.get_weights()\n",
    "                # weights.append(new_weights)\n",
    "                #print(f\"Episode {episode}: new weights = {new_weights}\")\n",
    "            # else:\n",
    "            #     print(f\"Episode {episode}: pas assez de données dans le replay buffer.\")\n",
    "            # print(values)\n",
    "            # if list_values:\n",
    "            #    # print(f'list values {list_values[-1]}')\n",
    "\n",
    "\n",
    "        traci.simulationStep()\n",
    "    print(f'episode : {episode}')\n",
    "    traci.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "best_score = 0\n",
    "reward = 0\n",
    "total_reward = 0\n",
    "list_values = []\n",
    "weights = []\n",
    "wait_times = []\n",
    "# replay_buffer = deque(maxlen=2000)\n",
    "\n",
    "\n",
    "sumoCmd = [sumo_gui_bin, \"-c\", simulConfig, \"--start\"]\n",
    "\n",
    "if traci.isLoaded():\n",
    "    traci.close()\n",
    "traci.start(sumoCmd)\n",
    "lane_ids =  traci.lane.getIDList()\n",
    "# print(lane_ids[0])\n",
    "# for lane in lane_ids:\n",
    "#     print(traci.lane.getLastStepVehicleNumber(lane))\n",
    "# north_lane = traci.lane.getLastStepVehicleNumber(\"N_0\")\n",
    "# south_lane = traci.lane.getLastStepVehicleNumber(\"S_0\")\n",
    "# east_lane = traci.lane.getLastStepVehicleNumber(\"E_0\")\n",
    "# west_lane = traci.lane.getLastStepVehicleNumber(\"W_0\")\n",
    "trafic_light_ids = traci.trafficlight.getIDList()\n",
    "\n",
    "state = np.array(get_state(lane_ids))\n",
    "action=-1\n",
    "# print(state)\n",
    "wait_times.append(0)\n",
    "for step in range(10000): ## TO CHANGED\n",
    "    #epsilon = max(1 - episode / 10, 0.01)\n",
    "    nom_du_feu= traci.trafficlight.getIDList()[0]\n",
    "\n",
    "    if step%500 == 0:\n",
    "        state=np.array(get_state(lane_ids))\n",
    "        action = epsilon_greedy_policy(state,0)*2\n",
    "        # action = np.random.randint(8)\n",
    "        print(\"action\", action)\n",
    "        #print(traci.trafficlight.getAllProgramLogics(nom_du_feu))\n",
    "        #print(traci.trafficlight.getAllProgramLogics(nom_du_feu)[0].phases[action])\n",
    "        traci.trafficlight.setPhase(trafic_light_ids[0],action)\n",
    "    traci.simulationStep()\n",
    "\n",
    "traci.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sumo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
